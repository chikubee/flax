{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "sst2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7 64-bit ('base': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.9.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "506677dbd00e645e3a2e7a7742f0bea2d4308eceaec0cc451debdc8fc3334b18"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Connect to GPU\n",
        "2. Mount Google Drive\n",
        "3. Clone the repository\n",
        "4. cd drive/My Drive/flax/examples/sst"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# cd drive/My Drive/flax/examples/sst"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Note: In Colab, above cell changed the working directory.\n",
        "!pwd"
      ],
      "outputs": [],
      "metadata": {
        "id": "xcXZ-F3_zBuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install SST-2 dependencies.\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install --upgrade git+https://github.com/google/flax.git\n",
        "!pip install --upgrade jax jaxlib"
      ],
      "outputs": [],
      "metadata": {
        "id": "qgUlFbSy_9q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Copyright 2022 Google LLC.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_sst_data(path, lower=True):\n",
        "  \"\"\"Loads an SST file as a TF dataset.\"\"\"\n",
        "  data = {'text': [], 'label': []}\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      line = line.lower() if lower else line\n",
        "      # We skip the below data fixes since they weren't used by the paper.\n",
        "      # line = line.replace(\"\\\\\", \"\")\n",
        "      # line = re.sub(\"\\\\\\\\\", \"\", line)\n",
        "      tokens = re.findall(r\"\\([0-9] ([^\\(\\)]+)\\)\", line)\n",
        "      label = int(line[1])\n",
        "      data['text'].append(' '.join(tokens))\n",
        "      data['label'].append(label)          \n",
        "    return tf.data.Dataset.from_tensor_slices(data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "08kWwdKZYZtG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_dataset = load_sst_data('data/trees/train.txt')\n",
        "val_dataset = load_sst_data('data/trees/dev.txt')\n",
        "test_dataset = load_sst_data('data/trees/test.txt')\n",
        "next(iter(train_dataset))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Copyright 2022 Google LLC.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "from typing import Any, Callable, Tuple\n",
        "from jax import numpy as jnp\n",
        "from flax.linen import recurrent\n",
        "from sst2.models import flip_sequences\n",
        "import jax \n",
        "from flax import linen as nn\n",
        "import functools\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Tuple[int]\n",
        "Dtype = Any\n",
        "Array = Any\n",
        "\n",
        "KERNEL_INIT = nn.linear.default_kernel_init\n",
        "RECURRENT_INIT = nn.initializers.orthogonal\n",
        "\n",
        "class MyLSTMCell(recurrent.RNNCellBase):\n",
        "  \"\"\"A PyTorch-compatible LSTM cell.\"\"\"\n",
        "  gate_fn: Callable[..., Any] = nn.sigmoid\n",
        "  activation_fn: Callable[..., Any] = nn.tanh\n",
        "  kernel_init: Callable[..., Array] = KERNEL_INIT\n",
        "  recurrent_kernel_init: Callable[..., Array] = RECURRENT_INIT()\n",
        "  bias_init: Callable[..., Array] = nn.initializers.zeros\n",
        "  dtype: Dtype = jnp.float32\n",
        "  param_dtype: Dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, carry, inputs):\n",
        "    \"\"\"Performs a single time step of the cell.\n",
        "    Args:\n",
        "      carry: the hidden state of the LSTM cell, a tuple (c, h),\n",
        "        initialized using `MyLSTMCell.initialize_carry`.\n",
        "      inputs: an ndarray with the input for the current time step.\n",
        "        All dimensions except the final are considered batch dimensions.\n",
        "    Returns:\n",
        "      A tuple with the new carry (c', h') and the output (h').\n",
        "    \"\"\"\n",
        "    c, h = carry\n",
        "    features = h.shape[-1]\n",
        "    \n",
        "    # Compute [h_i, h_f, h_g, h_o] at the same time for better performance.\n",
        "    dense_h = nn.Dense(\n",
        "        features=features * 4,\n",
        "        use_bias=True,\n",
        "        kernel_init=self.recurrent_kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        name='h', \n",
        "        dtype=self.dtype, \n",
        "        param_dtype=self.param_dtype)(h)\n",
        " \n",
        "    # Compute [i_i, i_f, i_g, i_o] at the same time for better performance.\n",
        "    dense_i = nn.Dense(\n",
        "        features=features * 4,\n",
        "        use_bias=True,  # dense_h already has a bias, but we follow PyTorch.\n",
        "        kernel_init=self.kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        name='i', \n",
        "        dtype=self.dtype, \n",
        "        param_dtype=self.param_dtype)(inputs)\n",
        "\n",
        "    # We sum each h_{i,f,g,o} with each i_{i,f,g,o} already now for performance.\n",
        "    summed_combined_projections = dense_i + dense_h\n",
        "\n",
        "    # Split into i = i_i + h_i, f = i_f + h_f, g = i_g + h_h, o = i_o + h_o.\n",
        "    i, g, f, o = jnp.split(summed_combined_projections, 4, axis=-1)\n",
        "\n",
        "    i = self.gate_fn(i)\n",
        "    f = self.gate_fn(f)\n",
        "    g = self.activation_fn(g)\n",
        "    o = self.gate_fn(o)\n",
        "\n",
        "    new_c = f * c + i * g\n",
        "    new_h = o * self.activation_fn(new_c)\n",
        "    return (new_c, new_h), new_h\n",
        "\n",
        "  @staticmethod\n",
        "  def initialize_carry(rng, batch_dims, size, init_fn=nn.initializers.zeros):\n",
        "    \"\"\"initialize the RNN cell carry.\n",
        "    Args:\n",
        "      rng: random number generator passed to the init_fn.\n",
        "      batch_dims: a tuple providing the shape of the batch dimensions.\n",
        "      size: the size or number of features of the memory.\n",
        "      init_fn: initializer function for the carry.\n",
        "    Returns:\n",
        "      An initialized carry for the given RNN cell.\n",
        "    \"\"\"\n",
        "    key1, key2 = jax.random.split(rng)\n",
        "    mem_shape = batch_dims + (size,)\n",
        "    return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  \"\"\"A simple unidirectional LSTM.\"\"\"\n",
        "\n",
        "  @functools.partial(\n",
        "      nn.transforms.scan,\n",
        "      variable_broadcast='params',\n",
        "      in_axes=1, out_axes=1,\n",
        "      split_rngs={'params': False})\n",
        "      \n",
        "  @nn.compact\n",
        "  def __call__(self, carry, x):\n",
        "    return MyLSTMCell(name='cell')(carry, x)\n",
        "\n",
        "  @staticmethod\n",
        "  def initialize_carry(batch_dims, hidden_size):\n",
        "    return MyLSTMCell.initialize_carry(\n",
        "        jax.random.PRNGKey(0), batch_dims, hidden_size)\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  \"\"\"A simple bi-directional LSTM.\"\"\"\n",
        "  hidden_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, lengths):\n",
        "    batch_size = inputs.shape[0]\n",
        "\n",
        "    # Forward LSTM.\n",
        "    initial_state = LSTM.initialize_carry((batch_size,), self.hidden_size)\n",
        "    _, forward_outputs = LSTM(name='lstm_fwd')(initial_state, inputs)\n",
        "    forward_final = forward_outputs[jnp.arange(inputs.shape[0]), lengths - 1]\n",
        "\n",
        "    # Backward LSTM.\n",
        "    reversed_inputs = flip_sequences(inputs, lengths)\n",
        "    initial_state = LSTM.initialize_carry((batch_size,), self.hidden_size)\n",
        "    _, backward_outputs = LSTM(name='lstm_bwd')(initial_state, reversed_inputs)\n",
        "    backward_final = backward_outputs[jnp.arange(inputs.shape[0]), lengths - 1]\n",
        "\n",
        "    # Concatenate the forward and backward representations.\n",
        "    # `outputs` is shaped [B, T, 2*D] and contains all (h) vectors across time.\n",
        "    backward_outputs = flip_sequences(backward_outputs, lengths)\n",
        "    outputs = jnp.concatenate([forward_outputs, backward_outputs], -1)\n",
        "\n",
        "    return outputs, (forward_final, backward_final)\n",
        "\n",
        "\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "  hidden_size: int\n",
        "  embedding_size: int\n",
        "  vocab_size: int\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, lengths):\n",
        "    \"\"\"Embeds and encodes the inputs, and then predicts.\"\"\"\n",
        "    embedded = nn.Embed(\n",
        "        self.vocab_size, \n",
        "        features=self.embedding_size, \n",
        "        name='embedder')(\n",
        "            inputs)\n",
        "    _, (forward_final, backward_final) = BiLSTM(\n",
        "        self.hidden_size, \n",
        "        name='bilstm')(\n",
        "            embedded, lengths)\n",
        "    forward_output = nn.Dense(\n",
        "        self.output_size, use_bias=False, name='output_layer_fwd')(\n",
        "            forward_final)\n",
        "    backward_output = nn.Dense(\n",
        "        self.output_size, use_bias=False, name='output_layer_bwd')(\n",
        "            backward_final)\n",
        "    return forward_output + backward_output  # Logits."
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# SANITY CHECK\n",
        "model = BiLSTMClassifier(hidden_size=60, embedding_size=60, vocab_size=19538, output_size=5)\n",
        "x = np.array([[1, 0, 0], [2, 3, 0], [4, 5, 6]])\n",
        "lengths = np.array([1, 2, 3])\n",
        "variables = model.init(jax.random.PRNGKey(0), x, lengths)\n",
        "outputs = model.apply(variables, x, lengths)\n",
        "print('outputs shape:', jax.tree_map(np.shape, outputs))\n",
        "print('outputs:', outputs)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sst2.train import *\n",
        "# update train_and_evaluate to take custom datasets and vocab as input\n",
        "def train_and_evaluate(train_dataset, val_dataset, vocab, config: ml_collections.ConfigDict,\n",
        "                       workdir: str) -> TrainState:\n",
        "  \"\"\"Execute model training and evaluation loop.\n",
        "  Args:\n",
        "    config: Hyperparameter configuration for training and evaluation.\n",
        "    workdir: Directory where the tensorboard summaries are written to.\n",
        "  Returns:\n",
        "    The final train state that includes the trained parameters.\n",
        "  \"\"\"\n",
        "  # Use datasets.\n",
        "  train_batches = train_dataset.get_batches(config.batch_size)\n",
        "  eval_batches = val_dataset.get_batches(batch_size=config.batch_size)\n",
        "\n",
        "  # Keep track of vocab size in the config so that the embedder knows it.\n",
        "  config.vocab_size = len(vocab)\n",
        "\n",
        "  # Compile step functions.\n",
        "  train_step_fn = jax.jit(train_step)\n",
        "  eval_step_fn = jax.jit(eval_step)\n",
        "\n",
        "  # Create model and a state that contains the parameters.\n",
        "  rng = jax.random.PRNGKey(config.seed)\n",
        "  model = model_from_config(config)\n",
        "  state = create_train_state(rng, config, model)\n",
        "\n",
        "  summary_writer = tensorboard.SummaryWriter(workdir)\n",
        "  summary_writer.hparams(dict(config))\n",
        "\n",
        "  # Main training loop.\n",
        "  logging.info('Starting training...')\n",
        "  for epoch in range(1, config.num_epochs + 1):\n",
        "\n",
        "    # Train for one epoch.\n",
        "    rng, epoch_rng = jax.random.split(rng)\n",
        "    rngs = {'dropout': epoch_rng}\n",
        "    state, train_metrics = train_epoch(\n",
        "        train_step_fn, state, train_batches, epoch, rngs)\n",
        "\n",
        "    # Evaluate current model on the validation data.\n",
        "    eval_metrics = evaluate_model(eval_step_fn, state, eval_batches, epoch)\n",
        "\n",
        "    # Write metrics to TensorBoard.\n",
        "    summary_writer.scalar('train_loss', train_metrics.loss, epoch)\n",
        "    summary_writer.scalar(\n",
        "        'train_accuracy',\n",
        "        train_metrics.accuracy * 100,\n",
        "        epoch)\n",
        "    summary_writer.scalar('eval_loss', eval_metrics.loss, epoch)\n",
        "    summary_writer.scalar(\n",
        "        'eval_accuracy',\n",
        "        eval_metrics.accuracy * 100,\n",
        "        epoch)\n",
        "\n",
        "  summary_writer.flush()\n",
        "  return state"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tftext\n",
        "from typing import Iterable, Sequence\n",
        "from itertools import chain\n",
        "from sst2 import vocabulary\n",
        "from sst2.input_pipeline import TextDataset, vocab_to_hashtable, AUTOTUNE, text\n",
        "\n",
        "def get_tokenized_sequences(\n",
        "        dataset: tf.data.Dataset,\n",
        "        tokenizer: tftext.Tokenizer = tftext.WhitespaceTokenizer(),\n",
        "        input_key: str = 'text') -> Iterable[Sequence[bytes]]:\n",
        "  \"\"\"Returns tokenized sequences for vocabulary building.\"\"\"\n",
        "  dataset = dataset.map(\n",
        "      lambda example: tokenizer.tokenize(example[input_key]),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  for sentence in tfds.as_numpy(dataset):\n",
        "    yield sentence\n",
        "\n",
        "# Iterate over the training, test, and dev datasets. (In this order!)\n",
        "# Add each word that you see to the vocabulary if you haven't seen it before.\n",
        "# You should end up with 19538 words if you do this correctly. \n",
        "# Verify that your vocab matches the serialized vocab from the repo https://github.com/ArrasL/LRP_for_LSTM.\n",
        "tokenized_sequences_train = get_tokenized_sequences(train_dataset)\n",
        "tokenized_sequences_test = get_tokenized_sequences(test_dataset)\n",
        "tokenized_sequences_val = get_tokenized_sequences(val_dataset)\n",
        "\n",
        "generator = chain(tokenized_sequences_train, tokenized_sequences_val, tokenized_sequences_test)\n",
        "\n",
        "# Builds the vocabulary from the tokenized sequences.\n",
        "# A token needs to appear at least 3 times to be in the vocabulary. You can\n",
        "# play with this. It is there to make sure we don't overfit on rare words.\n",
        "vocab = vocabulary.Vocabulary(\n",
        "    tokenized_sequences=generator, min_freq=1)\n",
        "vocab.save('vocab.txt')\n",
        "\n",
        "class TextDatasetSST(TextDataset):\n",
        "    def __init__(self, dataset: tf.data.Dataset, vocab_path: str = 'vocab.txt',\n",
        "               tokenizer: text.Tokenizer = text.WhitespaceTokenizer()):\n",
        "      \"\"\"Initializes the SST data source.\"\"\"\n",
        "      self.dataset = dataset\n",
        "      self.text_feature_name = 'text'\n",
        "      self.label_feature_name = 'label'\n",
        "\n",
        "      # Load the vocabulary.\n",
        "      self.vocab = vocabulary.Vocabulary(vocab_path=vocab_path)\n",
        "\n",
        "      # Convert the sentences to sequences of token IDs and compute length.\n",
        "      self.tokenizer = tokenizer\n",
        "      self.tf_vocab = vocab_to_hashtable(self.vocab, unk_idx=self.vocab.unk_idx)\n",
        "      self.examples = self.dataset.map(\n",
        "          self.prepare_example, num_parallel_calls=AUTOTUNE).cache()\n",
        "    \n",
        "    @property\n",
        "    def padded_shapes(self):\n",
        "      \"\"\"The padded shapes used by batching functions.\"\"\"\n",
        "      # None means variable length; pads to the longest sequence in the batch.\n",
        "      return {'token_ids': [None], 'label': [], 'length': []}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_ds = TextDatasetSST(train_dataset)\n",
        "val_ds = TextDatasetSST(val_dataset)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Get a live update during training - use the \"refresh\" button!\n",
        "# (In Jupyter[lab] start \"tensorboard\" in the local directory instead.)\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir=."
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time\n",
        "from configs import default as config_lib\n",
        "config = config_lib.get_config()\n",
        "model_name = 'sst_bilstm'\n",
        "start_time = time.time()\n",
        "optimizer = train_and_evaluate(train_ds, val_ds, config, workdir=f'./models/{model_name}')\n",
        "logging.info('Walltime: %f s', time.time() - start_time)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  #@markdown You can upload the training results directly to https://tensorboard.dev\n",
        "  #@markdown\n",
        "  #@markdown Note that everbody with the link will be able to see the data.\n",
        "  upload_data = 'yes' #@param ['yes', 'no']\n",
        "  if upload_data == 'yes':\n",
        "    !tensorboard dev upload --one_shot --logdir ./models --name 'Flax examples/sst'"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}